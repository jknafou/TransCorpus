# TransCorpus
**TransCorpus** is a scalable, production-ready API and CLI toolkit for large-scale parallel translation, preprocessing, and corpus management. It supports multi-GPU translation, robust checkpointing, and safe concurrent downloads, making it ideal for research and industry-scale machine translation workflows.

# Features
- üöÄ **Multi-GPU** and multi-process translation
- üì¶ Corpus downloading and preprocessing
- üîí Safe, **resumable**, and concurrent file downloads
- üß© **Split** and checkpoint management for large corpora
- üõ†Ô∏è Easy deployment and extensibility
- üñ•Ô∏è Cross-platform: Linux, macOS, Windows

# Quick Start
1. Clone and Install
```bash
git clone https://github.com/jknafou/TransCorpus.git
cd TransCorpus
UV_INDEX_STRATEGY=unsafe-best-match rye sync
source .venv/bin/activate
```
2. Download a Corpus

```bash
transcorpus download-corpus [corpus_name]
```
3. Preprocess the corpus by splits
```bash
transcorpus preprocess [corpus_name] [language] --num-split 100
```
4. Translate (and preprocess if not done) the corpus by split
```bash
transcorpus translate [corpus_name] [language] --num-split 100
```
5. Preview a corpus with two languages next to each other:
```bash
transcorpus preview [corpus_name] [language1] Opt[language2]
```

<br>
<div align="center">
  <img src="https://transcorpus.s3.text-analytics.ch/sidepreview.png" alt="Example of two languages next to each other" width="500"/>
</div>
<br>

A demo mode can be tested using the -d flag for each command.

## Preprocess and Translate (Multi-GPU Example)
The following example translates the bio corpus (PubMed) of about 30GB, preprocessing it with 4 parallel workers, while translating each available split with two GPUs of different sizes. It can easily be modified to one needs. When deployed on an HPC cluster, for example with SLURM, it will automatically resume from where it left off in the previous run. With shared memory, multiple GPUs from different nodes can work simultaneously.
```bash
# Preprocess with 4 workers iteratively, split into 20 parts (here in demo mode)
./example/multi_GPU.sh bio de 4 20

```

# Research-Proven Performance
## Paper to be accepted at EMNLP2025
TransCorpus enables the training of state-of-the-art language models through synthetic translation. For example, TransBERT achieved superior performance by leveraging corpus translation with this toolkit. The paper detailing these results has been accepted at EMNLP2025. üìù [Current Paper Version](https://transbert.s3.text-analytics.ch/TransBERT.pdf). If you use this toolkit, please cite:

```text
  @inproceedings{
    knafou2025transbert,
    title={Trans{BERT}: A Framework for Synthetic Translation in Domain-Specific Language Modeling},
    author={Julien Knafou and Luc Mottin and Ana{\"\i}s Mottaz and Alexandre Flament and Patrick Ruch},
    booktitle={The 2025 Conference on Empirical Methods in Natural Language Processing},
    year={2025},
    url={https://transbert.s3.text-analytics.ch/TransBERT.pdf}
  }
```

## üß¨ Pretrained Models
Looking for pretrained models built with TransCorpus?
Check out [TransBERT-bio-fr on Hugging Face ü§ó](https://huggingface.co/jknafou/TransBERT-bio-fr), a French biomedical language model trained entirely on synthetic translations generated by this toolkit. Also available, [TransCorpus-bio-fr on Hugging Face ü§ó](https://huggingface.co/datasets/jknafou/TransCorpus-bio-fr)


## New corpus upload
One can easily add its own corpus (along with a demo) to the repo following the same schema of ```domains.json```:
```json
    "bio": {
        "database": {
            "file": "https://transcorpus.s3.text-analytics.ch/bibmed.tar.gz"
        },
        "corpus": {
            "file":
                "https://transcorpus.s3.text-analytics.ch/title_abstract_en.txt"
            ,
            "demo":
                "https://transcorpus.s3.text-analytics.ch/1k_sample.txt"
        },
        "id": {
            "file":
                "https://transcorpus.s3.text-analytics.ch/PMID.txt"
            ,
            "demo":
                "https://transcorpus.s3.text-analytics.ch/PMID_1k_sample.txt"
        },
        "language": "en"
    }
```
Where each line of the corpus is a different document. For the moment, a life-science corpus is available comprising about 28GB of raw text, 22M of abstracts from PubMed. The database it is made of can also be downloaded using ```transcorpus download-database bio```.

# Deployment
## Requirements:

- Python 3.10+
- rye (for dependency management)
- CUDA-enabled GPUs (for multi-GPU translation)

## Contributing
Pull requests and issues are welcome!

## License
MIT License

## Acknowledgements
- Swiss AI Center
- fairseq
- PyTorch
- rye

**TransCorpus** makes large-scale, robust translation easy and reproducible.
